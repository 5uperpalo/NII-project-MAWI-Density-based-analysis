{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "MAWI traces contain packet traces of MAWI traffic captured as a part of research at NII Tokyo at FukudaLab. First, we have to decompress files and divide them to 1s time windows. \n",
    "The window size was chosen as a smallest time measure separating anomalies identified by MAWIlab project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset splitting into time windows\n",
    "Make sure that editcap is installed on the local machine, if not install it !\n",
    "\n",
    "    which editcap\n",
    "    sudo apt-get install wireshark-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# SPLIT\n",
    "root_dir=\"/home/.../052016/\"\n",
    "for filename in $root_dir/20*\n",
    "do\n",
    "        # get filename from \"fullpath filename\":\n",
    "        #https://stackoverflow.com/questions/22727107/how-to-find-the-last-field-using-cut/22727211\n",
    "        file=$filename | rev | cut -d'/' -f 1 | rev\n",
    "        # -i X ; X - number of seconds\n",
    "        editcap -i 1 $filename \"$root_dir/splitted_1s/$file\"\n",
    "        echo $file\n",
    "        #rm $filename\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Field extraction from the splitted traces\n",
    "In the same local node, we run the tshark network protocol analyzer to decode the trace content and extract the following basic features: \n",
    "\n",
    "* frame.time \n",
    "* frame.len \n",
    "* ip.proto\n",
    "* ip.len\n",
    "* ip.ttl\n",
    "* ip.version\n",
    "* ip.flags\n",
    "* ip.flags.mf\n",
    "* ip.frag_offset\n",
    "* ip.src \n",
    "* ip.dst\n",
    "* icmp.type\n",
    "* icmp.code\n",
    "* tcp.dstport \n",
    "* tcp.srcport \n",
    "* tcp.flags\n",
    "* tcp.flags.ack\n",
    "* tcp.flags.cwr\n",
    "* tcp.flags.fin\n",
    "* tcp.flags.ecn\n",
    "* tcp.flags.ns\n",
    "* tcp.flags.push\n",
    "* tcp.flags.syn\n",
    "* tcp.flags.urg\n",
    "* tcp.flags.reset\n",
    "* tcp.len\n",
    "* tcp.window_size\n",
    "* udp.srcport\n",
    "* udp.dstport\n",
    "\n",
    "\n",
    "Note:\n",
    "\n",
    "Make sure tshark is installed on the system!\n",
    "   \n",
    "    which tshark\n",
    "    sudo apt-get install tshark\n",
    "\n",
    "NOTE:\n",
    "It is a good idea to run the script with \"nohup SCRIPT_NAME &\" as it migth take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/.../052016\"\n",
    "mkdir -p \"$root_dir/splitted_1s/extracted_1/\"\n",
    "for filename in $root_dir/splitted_1s/_*\n",
    "do\n",
    "#        SUBSTRING=$(echo $filename | rev | cut -d'/' -f 1 | rev )\n",
    "        SUBSTRING=$(echo $filename | rev | cut -d'/' -f 1 | rev | cut -d'_' -f 3)\n",
    "        echo $SUBSTRING\n",
    "        tshark -T fields -n -r $filename -E aggregator=, -e frame.time -e frame.len -e ip.proto -e ip.len -e ip.ttl -e ip.version -e ip.flags -e ip.flags.mf -e ip.frag_offset -e ip.src -e ip.dst -e icmp.type -e icmp.code -e tcp.dstport -e tcp.srcport -e tcp.flags -e tcp.flags.ack -e tcp.flags.cwr -e tcp.flags.fin -e tcp.flags.ecn -e tcp.flags.ns -e tcp.flags.push -e tcp.flags.syn -e tcp.flags.urg -e tcp.flags.reset -e tcp.len -e tcp.window_size -e udp.srcport -e udp.dstport > $root_dir/splitted_1s/extracted_1s/$SUBSTRING\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The splits containing these features in human readable format are then uploaded HDFS for further feature extraction in Python SPARK distributed computing environment. Each file is autonomously named by editcap in the format: YYYYMMDDHHmmss.\n",
    "\n",
    "## 1.3 Copy the extracted fields to hdfs for distributed processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# COPY TO HDFS\n",
    "\n",
    "hdfs_dir=\"/user/.../052016/extracted_1s/\"\n",
    "hdfs dfs -mkdir -p $hdfs_dir\n",
    "for filename in $root_dir/splitted_1min/extracted_1min/*\n",
    "do\n",
    "        echo $filename\n",
    "        hdfs dfs -copyFromLocal $filename $hdfs_dir\n",
    "        #rm $filename\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Table of features we are going to extract from splits:\n",
    "\n",
    "| Field           | Feature                     | Description                 |\n",
    "|-----------------|-----------------------------|-----------------------------|\n",
    "| Tot. volume     | # pkts                      | num. packets                |\n",
    "|    &nbsp;       | # bytes                     | num. bytes                  |\n",
    "| PKT size        | pkt_h                       | H(PKT)                      |\n",
    "|    &nbsp;       | pkt_{min,avg,max,std}       | min/avg/max/std PKT         |\n",
    "|    &nbsp;       | pkt_p{1,2,5,...,95,97,99}   | percentiles                 |\n",
    "| IP PKT size     | iplen_h                     | H(IPlen)                    |\n",
    "|    &nbsp;       | iplen_{min,avg,max,std}     | min/avg/max/std IPlen       |\n",
    "|    &nbsp;       | iplen_p{1,2,5,...,95,97,99} | percentiles                 |\n",
    "| IP TTL          | ttl_h                       | H(TTL)                      |\n",
    "|    &nbsp;       | ttl_{min,avg,max,std}       | min/avg/max/std TTL         |\n",
    "|    &nbsp;       | ttl_p{1,2,5,...,95,97,99}   | percentiles                 |\n",
    "| IP Proto        | % icmp/tcp/udp/gre          | share of IP protocols       |\n",
    "| IPv4/IPv6       | % IPv4/IPv6                 | share of IPv4/IPv6 pkts.    |\n",
    "|    &nbsp;       | h_IP_src/dst_octet          | H(IP_octets)                |\n",
    "| TCP PKT size    | tcp_h                       | H(TCP)                      |\n",
    "|    &nbsp;       | tcp_{min,avg,max,std}       | min/avg/max/std TCP         |\n",
    "|    &nbsp;       | tcp_p{1,2,5,...,95,97,99}   | percentiles                 |\n",
    "| TCP WIN size    | win_h                       | H(TCPW)                     |\n",
    "|    &nbsp;       | win_{min,avg,max,std}       | min/avg/max/std TCPW        |\n",
    "|    &nbsp;       | win_p{1,2,5,...,95,97,99}   | percentiles                 |\n",
    "| TCP flags (byte)| % SYN/ACK/PSH/...           | share of TCP flags          |\n",
    "| TCP/UDP ports   | # well-known                | share of well-known         |\n",
    "| (src/dst)       | # registered                | share of registered         |\n",
    "|    &nbsp;       | # dynamic                   | share of dynamic            |\n",
    "|    &nbsp;       | # remote-access             | share of remote-access      |\n",
    "|    &nbsp;       | # mail                      | share of mail               |\n",
    "|    &nbsp;       | # networking                | share of networking         |\n",
    "|    &nbsp;       | # document-retrieval        | share of document retrieval |\n",
    "|  IP MF flag +   | # of frag. pkts             | share of of frag. packets   |\n",
    "| IP frag offset  |       &nbsp;                |          &nbsp;             |\n",
    "|   ICMP type     | # of icmp type 0            | share of echo reply packets |\n",
    "|    &nbsp;       | # of icmp type 3            | share of dest. unr. pkts.   |\n",
    "|    &nbsp;       | # of icmp type 5            | share of redirect mes. pkts.|\n",
    "|    &nbsp;       | # of icmp type 8            | share of echo request pkts  |\n",
    "|    &nbsp;       | h_icmp_type                 | H(icmp_type)                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Port number explanation in extracted features\n",
    "\n",
    "remote_access\n",
    "* tcp\n",
    "    * 22\t\tssh\t\t\t\tSSH - Secure SHell\n",
    "    * 23\t\ttelnet\t\t\tTelnet - Insecure remote login - RFC 854\n",
    "    * 992\t\ttelnets\t\t\tTelnet over SSL\n",
    "    * 513\t\trlogin\t\t\tremote login - RFC 1282\n",
    "\n",
    "mail\n",
    "* tcp\n",
    "    * 25\t\tsmtp\t\t\tSMTP - Simple Mail Transfer Protocol - RFC 2821 (See also RFC 1869)\n",
    "    * 110\t\tpop3\t\t\tPOP3 - Post Office Protocol version 3 (popular e-mail protocol) - RFC 1939\n",
    "    * 143\t\timap\t\t\tIMAP - Internet Message Access Protocol (A common e-mail protocol)\n",
    "    * 993\t\timaps\t\t\tIMAP over SSL\n",
    "    * 995\t\tpop3s\t\t\tPOP-3 over SSL\n",
    "* udp\n",
    "    * 512\t\tbiff\t\t\tBiff - new mail notification\n",
    "\n",
    "networking\n",
    "* tcp\n",
    "    * 43\t\twhois\t\t\tWhois - query/response system, usually used for domain name info - RFC 3912\n",
    "    * 67\t\n",
    "    * 68\n",
    "    * 137\t\tnetbios-ns\t\tNetBIOS - Network Basic Input Output System(Name Service)\n",
    "    * 138\t\tnetbios-dgm\t\tNetBIOS - Network Basic Input Output System(Datagram Service)\n",
    "    * 139\t\tnetbios-ssn\t\tNetBIOS - Network Basic Input Output System(session service)\n",
    "    * 161\t\tsnmp\t\t\tSNMP - Simple Network Management Protocol - RFC 1157\n",
    "    * 162\t\tsnmp-trap\t\tSNMP - Simple Network Management Protocol(traps) - RFC 1157\n",
    "    * 179\t\tbgp\t\t\t\tBGP - Border Gateway Protocol - RFC 1771\n",
    "* udp\n",
    "    * 53\t\tdns\t\t\t\tDNS - Domain Name System - RFC 1035\n",
    "    * 67\t\tdhcp\t\t\tDHCP - Dynamic Host Configuration Protocol - RFC 1541\n",
    "    * 68\n",
    "    * 137\t\tnetbios-ns\t\tNetBIOS - Network Basic Input Output System(Name Service)\n",
    "    * 138\t\tnetbios-dgm\t\tNetBIOS - Network Basic Input Output System(Datagram Service)\n",
    "    * 139\t\tnetbios-ssn\t\tNetBIOS - Network Basic Input Output System(session service)\n",
    "    * 161\t\tsnmp\t\t\tSNMP - Simple Network Management Protocol - RFC 1157\n",
    "    * 162\t\tsnmp-trap\t\tSNMP - Simple Network Management Protocol(traps) - RFC 1157\n",
    "\n",
    "document_retrieval\n",
    "* tcp\n",
    "    * 20\t\tftp-data\t\tFTP - File Transfer Protocol(data) - RFC 959\n",
    "    * 21\t\tftp\t\t\t\tFTP - File Transfer Protocol(control) - RFC 959\n",
    "    * 70\t\tgopher\t\t\tGopher - A precursor to HTTP - RFC 1436\n",
    "    * 80\t\thttp\t\t\tHTTP - HyperText Transfer Protocol - RFC 2616\n",
    "    * 443\t\thttps           HTTP over TLS/SSL\n",
    "    * 445\t\tsmb\t\t\t\tSamba/SMB - Server Message Block - Microsoft Windows filesharing\n",
    "    * 540\t\tuucp\t\t\tUUCP - Unix to Unix Copy\n",
    "    * 989\t\tftps-data\t\tFTP over SSL(data)\n",
    "    * 990\t\tftps\t\t\tFTP over SSL(control)\n",
    "* udp\n",
    "    * 69\t\ttftp\t\t\tTFTP - Trivial File Transfer Protocol - used for bootstrapping - RFC 1350\n",
    "    * 445\t\tsmb\t\t\t\tSamba/SMB - Server Message Block - Microsoft Windows filesharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Session initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "done with startup\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/home/big-dama/anaconda3/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/home/big-dama/anaconda3/bin/python\"\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-oracle/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"pyspark-shell\"\n",
    "\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark import sql\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext, HiveContext, DataFrameWriter\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, count, sum\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import math\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from operator import add\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# described here https://medium.com/@achilleus/spark-session-10d0d66d1d24\n",
    "print(\"starting\")\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName(\"darknet_feature_extraction_without_hive\")\\\n",
    "                    .master(\"yarn\")\\\n",
    "                    .config(\"spark.submit.deployMode\",\"client\")\\\n",
    "                    .config(\"spark.executor.memory\",\"20g\")\\\n",
    "                    .config(\"spark.driver.memory\",\"20g\")\\\n",
    "                    .config('spark.sql.autoBroadcastJoinThreshold','-1')\\\n",
    "                    .enableHiveSupport()\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(\"done with startup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_dir = '/user/big-dama/pavol/mawi_data_052016/extracted_mawi_052016/'\n",
    "output_loc = '/user/big-dama/pavol/mawi_data_052016/extracted_mawi_052016_features/'\n",
    "\n",
    "file_dir = subprocess.run([\"hdfs\", \"dfs\", \"-ls\", hdfs_dir], stdout=subprocess.PIPE)\n",
    "file_dir = str(file_dir.stdout)\n",
    "file_dir = file_dir.split('\\\\n')\n",
    "\n",
    "file_full_path_list = []\n",
    "\n",
    "i = 0\n",
    "for file_name in file_dir:\n",
    "    if ((i != 0) and (\"/\" in file_name)):\n",
    "        splits = file_name.split(\" \")\n",
    "        file_full_path_list.append(splits[-1].lstrip('\\n').strip())\n",
    "    i = i+1\n",
    "\n",
    "output_loc_ls = subprocess.run([\"hdfs\", \"dfs\", \"-ls\", output_loc+'/'], stdout=subprocess.PIPE)\n",
    "\n",
    "if not output_loc_ls.stdout:\n",
    "    processed_files=list()\n",
    "else:\n",
    "    processed_files_df = spark.read.format(\"csv\").option(\"header\", \"false\").option(\"delimiter\", \"\\t\").load(output_loc+\"/*\")\n",
    "    processed_files = [str(row._c0) for row in processed_files_df.collect()]\n",
    "\n",
    "files_to_process = list(set(file_full_path_list) - set(processed_files))\n",
    "\n",
    "schema = StructType([StructField('timestamp',StringType(),True)\\\n",
    "                     , StructField('frame_len',DoubleType(),True)\\\n",
    "                     , StructField('ip_proto',DoubleType(),True)\\\n",
    "                     , StructField('ip_len',DoubleType(),True)\\\n",
    "                     , StructField('ip_ttl',DoubleType(),True)\\\n",
    "                     , StructField('ip_version',DoubleType(),True)\\\n",
    "                     , StructField('ip_flags',StringType(),True)\\\n",
    "                     , StructField('ip_flags_mf',IntegerType(),True)\\\n",
    "                     , StructField('ip_frag_offset',IntegerType(),True)\\\n",
    "                     , StructField('ip_src',StringType(),True)\\\n",
    "                     , StructField('ip_dst',StringType(),True)\\\n",
    "                     , StructField('icmp_type',DoubleType(),True)\\\n",
    "                     , StructField('icmp_code',StringType(),True)\\\n",
    "                     , StructField('tcp_dstport',IntegerType(),True)\\\n",
    "                     , StructField('tcp_srcport',IntegerType(),True)\\\n",
    "                     , StructField('tcp_flags',StringType(),True)\\\n",
    "                     , StructField('tcp_flags_ack',DoubleType(),True)\\\n",
    "                     , StructField('tcp_flags_cwr',DoubleType(),True)\\\n",
    "                     , StructField('tcp_flags_fin',DoubleType(),True)\\\n",
    "                     , StructField('tcp_flags_ecn',DoubleType(),True)\\\n",
    "                     , StructField('tcp_flags_ns',DoubleType(),True)\\\n",
    "                     , StructField('tcp_flags_push',DoubleType(),True)\\\n",
    "                     , StructField('tcp_flags_syn',DoubleType(),True)\\\n",
    "                     , StructField('tcp_flags_urg',DoubleType(),True)\\\n",
    "                     , StructField('tcp_flags_reset',DoubleType(),True)\\\n",
    "                     , StructField('tcp_len',DoubleType(),True)\\\n",
    "                     , StructField('tcp_winsize',DoubleType(),True)\\\n",
    "                     , StructField('udp_srcport',IntegerType(),True)\\\n",
    "                     , StructField('udp_dstport',IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+-------------------+--------------+--------------+------------------+------------------+-----------------+----------------------------------------------------------+------------------+-----------+-----------+-----------------+----------------+-----------------+--------------------------------------------------------+-----------------+-----------+-----------+-----------------+-----------------+-----------------+--------------------------------------------------------+------------------+------------+------------+-----------------+------------------+-----------------+---------------------------------------------------------+------------------+----------------+----------------+------------------+-------------------+-------------------+-------------------------------------------------------------+--------------------+------------------+------------------+-------------------+-------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+--------------------------------+-------------------------------+--------------------------------+-------------------------------+--------------------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+---------------------------+--------------------------+---------------------------+--------------------------+---------------------------+--------------------------+---------------------------+--------------------------+--------------------------+-------------------------+----------------------------+---------------------------+---------------------------+--------------------------+---------------------------+--------------------------+-----------------------------+----------------------------+--------------------+--------+------------------+--------------------------------------------------------------------------------+-------------------------------------------------------------------------------+\n",
      "|           file_name|vol(frame_len)|num_pkts(frame_len)|min(frame_len)|max(frame_len)|    avg(frame_len)|    var(frame_len)|stddev(frame_len)|percentiles(1_2_5_10_15_20_25_50_75_90_95_97_99_frame_len)|entropy(frame_len)|min(ip_len)|max(ip_len)|      avg(ip_len)|     var(ip_len)|   stddev(ip_len)|percentiles(1_2_5_10_15_20_25_50_75_90_95_97_99_(ip_len)|  entropy(ip_len)|min(ip_ttl)|max(ip_ttl)|      avg(ip_ttl)|      var(ip_ttl)|   stddev(ip_ttl)|percentiles(1_2_5_10_15_20_25_50_75_90_95_97_99_(ip_ttl)|   entropy(ip_ttl)|min(tcp_len)|max(tcp_len)|     avg(tcp_len)|      var(tcp_len)|  stddev(tcp_len)|percentiles(1_2_5_10_15_20_25_50_75_90_95_97_99_(tcp_len)|  entropy(tcp_len)|min(tcp_winsize)|max(tcp_winsize)|  avg(tcp_winsize)|   var(tcp_winsize)|stddev(tcp_winsize)|percentiles(1_2_5_10_15_20_25_50_75_90_95_97_99_(tcp_winsize)|entropy(tcp_winsize)|entropy(tcp_flags)|         frac_ipv4|          frac_ipv6|frac_icmp_tcp_udp_gre(ip_proto)|num_icmp_tcp_udp_gre(ip_proto)|num_appport_ranges(tcp_dstport)|frac_appport_ranges(tcp_dstport)|num_appport_ranges(tcp_srcport)|frac_appport_ranges(tcp_srcport)|num_appport_ranges(udp_dstport)|frac_appport_ranges(udp_dstport)|num_appport_ranges(udp_srcport)|frac_appport_ranges(udp_srcport)|entropy_1st_IP_octet(ip_dst)|entropy_2nd_IP_octet(ip_dst)|entropy_3rd_IP_octet(ip_dst)|entropy_4th_IP_octet(ip_dst)|entropy_1st_IP_octet(ip_src)|entropy_2nd_IP_octet(ip_src)|entropy_3rd_IP_octet(ip_src)|entropy_4th_IP_octet(ip_src)|frac_tcpflag(tcp_flags_ack)|num_tcpflag(tcp_flags_ack)|frac_tcpflag(tcp_flags_cwr)|num_tcpflag(tcp_flags_cwr)|frac_tcpflag(tcp_flags_fin)|num_tcpflag(tcp_flags_fin)|frac_tcpflag(tcp_flags_ecn)|num_tcpflag(tcp_flags_ecn)|frac_tcpflag(tcp_flags_ns)|num_tcpflag(tcp_flags_ns)|frac_tcpflag(tcp_flags_push)|num_tcpflag(tcp_flags_push)|frac_tcpflag(tcp_flags_syn)|num_tcpflag(tcp_flags_syn)|frac_tcpflag(tcp_flags_urg)|num_tcpflag(tcp_flags_urg)|frac_tcpflag(tcp_flags_reset)|num_tcpflag(tcp_flags_reset)|           frac_frag|num_frag|entropy(icmp_type)|frac_echo_reply_destination_unreachable_redirect_message_echo_request(icmp_type)|num_echo_reply_destination_unreachable_redirect_message_echo_request(icmp_type)|\n",
      "+--------------------+--------------+-------------------+--------------+--------------+------------------+------------------+-----------------+----------------------------------------------------------+------------------+-----------+-----------+-----------------+----------------+-----------------+--------------------------------------------------------+-----------------+-----------+-----------+-----------------+-----------------+-----------------+--------------------------------------------------------+------------------+------------+------------+-----------------+------------------+-----------------+---------------------------------------------------------+------------------+----------------+----------------+------------------+-------------------+-------------------+-------------------------------------------------------------+--------------------+------------------+------------------+-------------------+-------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+--------------------------------+-------------------------------+--------------------------------+-------------------------------+--------------------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+---------------------------+--------------------------+---------------------------+--------------------------+---------------------------+--------------------------+---------------------------+--------------------------+--------------------------+-------------------------+----------------------------+---------------------------+---------------------------+--------------------------+---------------------------+--------------------------+-----------------------------+----------------------------+--------------------+--------+------------------+--------------------------------------------------------------------------------+-------------------------------------------------------------------------------+\n",
      "|/user/big-dama/pa...|   1.5114992E7|              45898|          60.0|        1514.0|329.31700727700553|293628.33077421243|541.8748294340792|                                      [60.0, 60.0, 60.0...| 1.861092712454572|       28.0|     1500.0|308.1070279689036|299893.646007394|547.6254614308889|                                    [32.0, 32.0, 32.0...|2.113656342145198|        1.0|      253.0|76.19835135376643|2557.392575080523|50.57066911837852|                                    [46.0, 48.0, 50.0...|2.5335029022906923|         0.0|      1460.0|710.2328879364679|493155.49211970443|702.2503058879394|                                     [0.0, 0.0, 0.0, 0...|1.9637915735846128|             0.0|        983040.0|21769.041939546598|6.161619154734316E9|    78495.981774447|                                         [11.0, 32.0, 94.0...|    4.69301627306209|0.8951444055137273|0.9873261205564142|0.01267387944358578|           [0.53630149227057...|          [24007,15591,4606...|           [5667.0, 3623.0, ...|            [0.35625825108442...|           [8553.0, 3223.0, ...|            [0.53768781039793...|           [2030.0, 2057.0, ...|            [0.42309295539808...|           [541.0, 3193.0, 1...|            [0.11275531471446...|          3.9817138765075994|           4.283425113267606|           4.786823759728161|           4.969556607136509|          2.8899262069553657|          3.0904731864723436|           3.454081178430477|           3.496994851162434|         0.8712216624685138|                   13835.0|       4.408060453400503...|                       7.0|       0.014294710327455919|                     227.0|       3.778337531486146E-4|                       6.0|                       0.0|                      0.0|         0.10850125944584382|                     1723.0|         0.1336272040302267|                    2122.0|                        0.0|                       0.0|         0.010453400503778338|                       166.0|2.178744171859340...|    10.0|0.5353301938121674|                                                            [0.20705540568582...|                                                              [4989,67,0,19018]|\n",
      "+--------------------+--------------+-------------------+--------------+--------------+------------------+------------------+-----------------+----------------------------------------------------------+------------------+-----------+-----------+-----------------+----------------+-----------------+--------------------------------------------------------+-----------------+-----------+-----------+-----------------+-----------------+-----------------+--------------------------------------------------------+------------------+------------+------------+-----------------+------------------+-----------------+---------------------------------------------------------+------------------+----------------+----------------+------------------+-------------------+-------------------+-------------------------------------------------------------+--------------------+------------------+------------------+-------------------+-------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+--------------------------------+-------------------------------+--------------------------------+-------------------------------+--------------------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+---------------------------+--------------------------+---------------------------+--------------------------+---------------------------+--------------------------+---------------------------+--------------------------+--------------------------+-------------------------+----------------------------+---------------------------+---------------------------+--------------------------+---------------------------+--------------------------+-----------------------------+----------------------------+--------------------+--------+------------------+--------------------------------------------------------------------------------+-------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"string\", PandasUDFType.GROUPED_AGG)\n",
    "def num_tcp_appport_cat(labels):\n",
    "    well_known = np.linspace(0,1023,1024)\n",
    "    registered_ports = np.linspace(1024,49151,48128)\n",
    "    dynamic_ports = np.linspace(49152,65535,16384)\n",
    "\n",
    "    remote_access_tcp = [22,23,992,513]\n",
    "    mail_tcp = [25,110,143,993,995]\n",
    "    networking_tcp = [43,67,68,137,138,139,161,162,179]\n",
    "    document_retrieval_tcp = [20,21,70,80,443,445,540,989,990]\n",
    "\n",
    "\n",
    "    # filter port number types that are as index in values_counts result\n",
    "    list_of_ports_in_table = labels.value_counts(normalize=False).index.tolist()\n",
    "    # port ranges\n",
    "    if any(i in well_known for i in list_of_ports_in_table) :\n",
    "        sum_well_known = np.sum(labels.value_counts(normalize=False).loc[list(well_known)].dropna().values)\n",
    "    else:\n",
    "        sum_well_known = float(0)\n",
    "\n",
    "    if any(i in registered_ports for i in list_of_ports_in_table) :\n",
    "        sum_registered_ports = np.sum(labels.value_counts(normalize=False).loc[list(registered_ports)].dropna().values)\n",
    "    else:\n",
    "        sum_registered_ports = float(0)\n",
    "\n",
    "    if any(i in dynamic_ports for i in list_of_ports_in_table) :\n",
    "        sum_dynamic_ports = np.sum(labels.value_counts(normalize=False).loc[list(dynamic_ports)].dropna().values)\n",
    "    else:\n",
    "        sum_dynamic_ports = float(0)\n",
    "    # service categories\n",
    "    if any(i in remote_access_tcp for i in list_of_ports_in_table) :\n",
    "        sum_remote_access_tcp = np.sum(labels.value_counts(normalize=False).loc[list(remote_access_tcp)].dropna().values)\n",
    "    else:\n",
    "        sum_remote_access_tcp = float(0)\n",
    "\n",
    "    if any(i in mail_tcp for i in list_of_ports_in_table) :\n",
    "        sum_mail_tcp = np.sum(labels.value_counts(normalize=False).loc[list(mail_tcp)].dropna().values)\n",
    "    else:\n",
    "        sum_mail_tcp = float(0)\n",
    "\n",
    "    if any(i in networking_tcp for i in list_of_ports_in_table) :\n",
    "        sum_networking_tcp = np.sum(labels.value_counts(normalize=False).loc[list(networking_tcp)].dropna().values)\n",
    "    else:\n",
    "        sum_networking_tcp = float(0)\n",
    "\n",
    "    if any(i in document_retrieval_tcp for i in list_of_ports_in_table) :\n",
    "        sum_document_retrieval_tcp = np.sum(labels.value_counts(normalize=False).loc[list(document_retrieval_tcp)].dropna().values)\n",
    "    else:\n",
    "        sum_document_retrieval_tcp = float(0)\n",
    "\n",
    "    res = list([sum_well_known, sum_registered_ports, sum_dynamic_ports, sum_remote_access_tcp, sum_mail_tcp, sum_networking_tcp, sum_document_retrieval_tcp])\n",
    "    return str(res)\n",
    "\n",
    "@pandas_udf(\"string\", PandasUDFType.GROUPED_AGG)\n",
    "def num_udp_appport_cat(labels):\n",
    "    well_known = np.linspace(0,1023,1024)\n",
    "    registered_ports = np.linspace(1024,49151,48128)\n",
    "    dynamic_ports = np.linspace(49152,65535,16384)\n",
    "\n",
    "    mail_udp = [512]\n",
    "    networking_udp = [53,67,68,137,138,139,161,162]\n",
    "    document_retrieval_udp = [69,445]\n",
    "\n",
    "    # filter port number types that are as index in values_counts result\n",
    "    list_of_ports_in_table = labels.value_counts(normalize=False).index.tolist()\n",
    "    # port ranges\n",
    "    if any(i in well_known for i in list_of_ports_in_table) :\n",
    "        sum_well_known = np.sum(labels.value_counts(normalize=False).loc[list(well_known)].dropna().values)\n",
    "    else:\n",
    "        sum_well_known = float(0)\n",
    "\n",
    "    if any(i in registered_ports for i in list_of_ports_in_table) :\n",
    "        sum_registered_ports = np.sum(labels.value_counts(normalize=False).loc[list(registered_ports)].dropna().values)\n",
    "    else:\n",
    "        sum_registered_ports = float(0)\n",
    "\n",
    "    if any(i in dynamic_ports for i in list_of_ports_in_table) :\n",
    "        sum_dynamic_ports = np.sum(labels.value_counts(normalize=False).loc[list(dynamic_ports)].dropna().values)\n",
    "    else:\n",
    "        sum_dynamic_ports = float(0)\n",
    "    # service categories\n",
    "\n",
    "    if any(i in mail_udp for i in list_of_ports_in_table) :\n",
    "        sum_mail_udp = np.sum(labels.value_counts(normalize=False).loc[list(mail_udp)].dropna().values)\n",
    "    else:\n",
    "        sum_mail_udp = float(0)\n",
    "\n",
    "    if any(i in networking_udp for i in list_of_ports_in_table) :\n",
    "        sum_networking_udp = np.sum(labels.value_counts(normalize=False).loc[list(networking_udp)].dropna().values)\n",
    "    else:\n",
    "        sum_networking_udp = float(0)\n",
    "\n",
    "    if any(i in document_retrieval_udp for i in list_of_ports_in_table) :\n",
    "        sum_document_retrieval_udp = np.sum(labels.value_counts(normalize=False).loc[list(document_retrieval_udp)].dropna().values)\n",
    "    else:\n",
    "        sum_document_retrieval_udp = float(0)\n",
    "\n",
    "    res = list([sum_well_known, sum_registered_ports, sum_dynamic_ports, sum_mail_udp, sum_networking_udp, sum_document_retrieval_udp])\n",
    "    return str(res)\n",
    "\n",
    "@pandas_udf(\"string\", PandasUDFType.GROUPED_AGG)\n",
    "def frac_tcp_appport_cat(labels):\n",
    "    well_known = np.linspace(0,1023,1024)\n",
    "    registered_ports = np.linspace(1024,49151,48128)\n",
    "    dynamic_ports = np.linspace(49152,65535,16384)\n",
    "\n",
    "    remote_access_tcp = [22,23,992,513]\n",
    "    mail_tcp = [25,110,143,993,995]\n",
    "    networking_tcp = [43,67,68,137,138,139,161,162,179]\n",
    "    document_retrieval_tcp = [20,21,70,80,443,445,540,989,990]\n",
    "\n",
    "    # filter port number types that are as index in values_counts result\n",
    "    list_of_ports_in_table = labels.value_counts(normalize=True).index.tolist()\n",
    "    # port ranges\n",
    "    if any(i in well_known for i in list_of_ports_in_table) :\n",
    "        sum_well_known = np.sum(labels.value_counts(normalize=True).loc[list(well_known)].dropna().values)\n",
    "    else:\n",
    "        sum_well_known = float(0)\n",
    "\n",
    "    if any(i in registered_ports for i in list_of_ports_in_table) :\n",
    "        sum_registered_ports = np.sum(labels.value_counts(normalize=True).loc[list(registered_ports)].dropna().values)\n",
    "    else:\n",
    "        sum_registered_ports = float(0)\n",
    "\n",
    "    if any(i in dynamic_ports for i in list_of_ports_in_table) :\n",
    "        sum_dynamic_ports = np.sum(labels.value_counts(normalize=True).loc[list(dynamic_ports)].dropna().values)\n",
    "    else:\n",
    "        sum_dynamic_ports = float(0)\n",
    "    # service categories\n",
    "    if any(i in remote_access_tcp for i in list_of_ports_in_table) :\n",
    "        sum_remote_access_tcp = np.sum(labels.value_counts(normalize=True).loc[list(remote_access_tcp)].dropna().values)\n",
    "    else:\n",
    "        sum_remote_access_tcp = float(0)\n",
    "\n",
    "    if any(i in mail_tcp for i in list_of_ports_in_table) :\n",
    "        sum_mail_tcp = np.sum(labels.value_counts(normalize=True).loc[list(mail_tcp)].dropna().values)\n",
    "    else:\n",
    "        sum_mail_tcp = float(0)\n",
    "\n",
    "    if any(i in networking_tcp for i in list_of_ports_in_table) :\n",
    "        sum_networking_tcp = np.sum(labels.value_counts(normalize=True).loc[list(networking_tcp)].dropna().values)\n",
    "    else:\n",
    "        sum_networking_tcp = float(0)\n",
    "\n",
    "    if any(i in document_retrieval_tcp for i in list_of_ports_in_table) :\n",
    "        sum_document_retrieval_tcp = np.sum(labels.value_counts(normalize=True).loc[list(document_retrieval_tcp)].dropna().values)\n",
    "    else:\n",
    "        sum_document_retrieval_tcp = float(0)\n",
    "\n",
    "    res = list([sum_well_known, sum_registered_ports, sum_dynamic_ports, sum_remote_access_tcp, sum_mail_tcp, sum_networking_tcp, sum_document_retrieval_tcp])\n",
    "    return str(res)\n",
    "\n",
    "@pandas_udf(\"string\", PandasUDFType.GROUPED_AGG)\n",
    "def frac_udp_appport_cat(labels):\n",
    "    well_known = np.linspace(0,1023,1024)\n",
    "    registered_ports = np.linspace(1024,49151,48128)\n",
    "    dynamic_ports = np.linspace(49152,65535,16384)\n",
    "\n",
    "    mail_udp = [512]\n",
    "    networking_udp = [53,67,68,137,138,139,161,162]\n",
    "    document_retrieval_udp = [69,445]\n",
    "\n",
    "    # filter port number types that are as index in values_counts result\n",
    "    list_of_ports_in_table = labels.value_counts(normalize=True).index.tolist()\n",
    "    # port ranges\n",
    "    if any(i in well_known for i in list_of_ports_in_table) :\n",
    "        sum_well_known = np.sum(labels.value_counts(normalize=True).loc[list(well_known)].dropna().values)\n",
    "    else:\n",
    "        sum_well_known = float(0)\n",
    "\n",
    "    if any(i in registered_ports for i in list_of_ports_in_table) :\n",
    "        sum_registered_ports = np.sum(labels.value_counts(normalize=True).loc[list(registered_ports)].dropna().values)\n",
    "    else:\n",
    "        sum_registered_ports = float(0)\n",
    "\n",
    "    if any(i in dynamic_ports for i in list_of_ports_in_table) :\n",
    "        sum_dynamic_ports = np.sum(labels.value_counts(normalize=True).loc[list(dynamic_ports)].dropna().values)\n",
    "    else:\n",
    "        sum_dynamic_ports = float(0)\n",
    "    # service categories\n",
    "\n",
    "    if any(i in mail_udp for i in list_of_ports_in_table) :\n",
    "        sum_mail_udp = np.sum(labels.value_counts(normalize=True).loc[list(mail_udp)].dropna().values)\n",
    "    else:\n",
    "        sum_mail_udp = float(0)\n",
    "\n",
    "    if any(i in networking_udp for i in list_of_ports_in_table) :\n",
    "        sum_networking_udp = np.sum(labels.value_counts(normalize=True).loc[list(networking_udp)].dropna().values)\n",
    "    else:\n",
    "        sum_networking_udp = float(0)\n",
    "\n",
    "    if any(i in document_retrieval_udp for i in list_of_ports_in_table) :\n",
    "        sum_document_retrieval_udp = np.sum(labels.value_counts(normalize=True).loc[list(document_retrieval_udp)].dropna().values)\n",
    "    else:\n",
    "        sum_document_retrieval_udp = float(0)\n",
    "\n",
    "    res = list([sum_well_known, sum_registered_ports, sum_dynamic_ports, sum_mail_udp, sum_networking_udp, sum_document_retrieval_udp])\n",
    "    return str(res)\n",
    "\n",
    "# safest solution to compute entropy from https://stackoverflow.com/questions/15450192/fastest-way-to-compute-entropy-in-python\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def entropy_udf(labels):\n",
    "    value,counts = np.unique(labels.dropna(), return_counts=True)\n",
    "    return entropy(counts)\n",
    "\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def frac_ipv4(labels):\n",
    "    res = labels.value_counts(normalize=True)\n",
    "    if float(4) in res.index:\n",
    "        return res.loc[float(4)]\n",
    "    else:\n",
    "        return float(0)\n",
    "\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def frac_ipv6(labels):\n",
    "    res = labels.value_counts(normalize=True)\n",
    "    if float(6) in res.index:\n",
    "        return res.loc[float(6)]\n",
    "    else:\n",
    "        return float(0)\n",
    "\n",
    "@pandas_udf(\"string\", PandasUDFType.GROUPED_AGG)\n",
    "def num_icmp_tcp_udp_gre(labels):\n",
    "    res = labels.value_counts(normalize=False)\n",
    "    if float(1) in res.index:\n",
    "        icmp = str(res.loc[float(1)])\n",
    "    else:\n",
    "        icmp = str(0)\n",
    "    if float(6) in res.index:\n",
    "        tcp = str(res.loc[float(6)])\n",
    "    else:\n",
    "        tcp = str(0)\n",
    "    if float(17) in res.index:\n",
    "        udp = str(res.loc[float(17)])\n",
    "    else:\n",
    "        udp = str(0)\n",
    "    if float(47) in res.index:\n",
    "        gre = str(res.loc[float(47)])\n",
    "    else:\n",
    "        gre = str(0)\n",
    "    return \"[\"+icmp+\",\"+tcp+\",\"+udp+\",\"+gre+\"]\"\n",
    "\n",
    "\n",
    "@pandas_udf(\"string\", PandasUDFType.GROUPED_AGG)\n",
    "def frac_icmp_tcp_udp_gre(labels):\n",
    "    res = labels.value_counts(normalize=True)\n",
    "    if float(1) in res.index:\n",
    "        icmp = str(res.loc[float(1)])\n",
    "    else:\n",
    "        icmp = str(0)\n",
    "    if float(6) in res.index:\n",
    "        tcp = str(res.loc[float(6)])\n",
    "    else:\n",
    "        tcp = str(0)\n",
    "    if float(17) in res.index:\n",
    "        udp = str(res.loc[float(17)])\n",
    "    else:\n",
    "        udp = str(0)\n",
    "    if float(47) in res.index:\n",
    "        gre = str(res.loc[float(47)])\n",
    "    else:\n",
    "        gre = str(0)\n",
    "    return \"[\"+icmp+\",\"+tcp+\",\"+udp+\",\"+gre+\"]\"\n",
    "\n",
    "@pandas_udf(\"string\", PandasUDFType.GROUPED_AGG)\n",
    "def num_echo_reply_destination_unreachable_redirect_message_echo_request(labels):\n",
    "    res = labels.value_counts(normalize=False)\n",
    "    if float(0) in res.index:\n",
    "        echo_reply = str(res.loc[float(0)])\n",
    "    else:\n",
    "        echo_reply = str(0)\n",
    "    if float(3) in res.index:\n",
    "        destination_unreachable = str(res.loc[float(3)])\n",
    "    else:\n",
    "        destination_unreachable = str(0)\n",
    "    if float(5) in res.index:\n",
    "        redirect_message = str(res.loc[float(5)])\n",
    "    else:\n",
    "        redirect_message = str(0)\n",
    "    if float(8) in res.index:\n",
    "        echo_request = str(res.loc[float(8)])\n",
    "    else:\n",
    "        echo_request = str(0)\n",
    "    return \"[\"+echo_reply+\",\"+destination_unreachable+\",\"+redirect_message+\",\"+echo_request+\"]\"\n",
    "\n",
    "@pandas_udf(\"string\", PandasUDFType.GROUPED_AGG)\n",
    "def frac_echo_reply_destination_unreachable_redirect_message_echo_request(labels):\n",
    "    res = labels.value_counts(normalize=True)\n",
    "    if float(0) in res.index:\n",
    "        echo_reply = str(res.loc[float(0)])\n",
    "    else:\n",
    "        echo_reply = str(0)\n",
    "    if float(3) in res.index:\n",
    "        destination_unreachable = str(res.loc[float(3)])\n",
    "    else:\n",
    "        destination_unreachable = str(0)\n",
    "    if float(5) in res.index:\n",
    "        redirect_message = str(res.loc[float(5)])\n",
    "    else:\n",
    "        redirect_message = str(0)\n",
    "    if float(8) in res.index:\n",
    "        echo_request = str(res.loc[float(8)])\n",
    "    else:\n",
    "        echo_request = str(0)\n",
    "    return \"[\"+echo_reply+\",\"+destination_unreachable+\",\"+redirect_message+\",\"+echo_request+\"]\"\n",
    "\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def frac_tcpflag(labels):\n",
    "    res = labels.value_counts(normalize=True)\n",
    "    if float(1) in res.index:\n",
    "        return res.loc[float(1)]\n",
    "    else:\n",
    "        return float(0)\n",
    "\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def num_tcpflag(labels):\n",
    "    res = labels.value_counts(normalize=False)\n",
    "    if float(1) in res.index:\n",
    "        return res.loc[float(1)]\n",
    "    else:\n",
    "        return float(0)\n",
    "\n",
    "# list of percentiles we wil compute and corresponfing name of the column\n",
    "percentiles = [0.01, 0.02, 0.05, 0.10, 0.15, 0.20, 0.25, 0.50, 0.75, 0.90, 0.95, 0.97, 0.99]\n",
    "percentiles_col_name = \"percentiles(1_2_5_10_15_20_25_50_75_90_95_97_99_\"\n",
    "\n",
    "\n",
    "for file_name in files_to_process:\n",
    "    try:\n",
    "        datadf = spark.read.format(\"csv\").option(\"header\", \"false\").schema(schema).option(\"delimiter\", \"\\t\").load(file_name)\n",
    "\n",
    "        # initial creation of feature_subset row which will be written to hive table after all features are calculated\n",
    "        features_subset = datadf.select(F.sum(\"frame_len\").alias(\"vol(frame_len)\"), F.count(\"frame_len\").alias(\"num_pkts(frame_len)\"), F.min(\"frame_len\"), F.max(\"frame_len\"), F.avg(\"frame_len\"), F.variance(\"frame_len\").alias(\"var(frame_len)\"), F.stddev(\"frame_len\").alias(\"stddev(frame_len)\") ).withColumn(\"file_name\", lit(file_name))\n",
    "        # about percentiles:\n",
    "        # https://support.treasuredata.com/hc/en-us/articles/360001457367-Hive-Built-in-Aggregate-Functions\n",
    "        datadf_percentiles = datadf.approxQuantile(\"frame_len\",percentiles, 0)\n",
    "        features_subset = features_subset.withColumn(percentiles_col_name+\"frame_len)\", lit(str(datadf_percentiles)))\n",
    "        entropy_result = datadf.agg(entropy_udf(\"frame_len\").alias(\"entropy(frame_len)\")).withColumn(\"file_name\", lit(file_name))\n",
    "        features_subset = features_subset.join(entropy_result,\"file_name\")\n",
    "\n",
    "        for name in [\"ip_len\", \"ip_ttl\", \"tcp_len\",\"tcp_winsize\"]:\n",
    "            features_subset = features_subset.join(datadf.select(F.min(name), F.max(name), F.avg(name), F.variance(name).alias(\"var(\"+name+\")\"), F.stddev(name).alias(\"stddev(\"+name+\")\")).withColumn(\"file_name\", lit(file_name)),\"file_name\")\n",
    "            datadf_percentiles = datadf.approxQuantile(name,percentiles, 0)\n",
    "            features_subset = features_subset.withColumn(percentiles_col_name+\"(\"+name+\")\", lit(str(datadf_percentiles)))\n",
    "            entropy_result = datadf.agg(entropy_udf(name).alias(\"entropy(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "            features_subset = features_subset.join(entropy_result,\"file_name\")\n",
    "\n",
    "        name = \"tcp_flags\"\n",
    "        entropy_result = datadf.agg(entropy_udf(name).alias(\"entropy(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "        features_subset = features_subset.join(entropy_result,\"file_name\")\n",
    "\n",
    "        name = \"ip_version\"\n",
    "        ipv4_frac_result = datadf.agg(frac_ipv4(name).alias(\"frac_ipv4\")).withColumn(\"file_name\", lit(file_name))\n",
    "        ipv6_frac_result = datadf.agg(frac_ipv6(name).alias(\"frac_ipv6\")).withColumn(\"file_name\", lit(file_name))\n",
    "        features_subset = features_subset.join(ipv4_frac_result,\"file_name\")\n",
    "        features_subset = features_subset.join(ipv6_frac_result,\"file_name\")\n",
    "\n",
    "        name = \"ip_proto\"\n",
    "        frac_icmp_tcp_udp_gre_res = datadf.agg(frac_icmp_tcp_udp_gre(name).alias(\"frac_icmp_tcp_udp_gre(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "        features_subset = features_subset.join(frac_icmp_tcp_udp_gre_res,\"file_name\")\n",
    "        num_icmp_tcp_udp_gre_res = datadf.agg(num_icmp_tcp_udp_gre(name).alias(\"num_icmp_tcp_udp_gre(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "        features_subset = features_subset.join(num_icmp_tcp_udp_gre_res,\"file_name\")\n",
    "\n",
    "        # TCP/UDP port categories\n",
    "\n",
    "        for name in [\"tcp_dstport\",\"tcp_srcport\"] :\n",
    "            num_tcp_appport_categories = datadf.agg(num_tcp_appport_cat(name).alias(\"num_appport_ranges(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "            frac_tcp_appport_categories = datadf.agg(frac_tcp_appport_cat(name).alias(\"frac_appport_ranges(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "            features_subset = features_subset.join(num_tcp_appport_categories,\"file_name\")\n",
    "            features_subset = features_subset.join(frac_tcp_appport_categories,\"file_name\")\n",
    "\n",
    "        for name in ['udp_dstport','udp_srcport']:\n",
    "            num_udp_appport_categories = datadf.agg(num_udp_appport_cat(name).alias(\"num_appport_ranges(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "            frac_udp_appport_categories = datadf.agg(frac_udp_appport_cat(name).alias(\"frac_appport_ranges(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "            features_subset = features_subset.join(num_udp_appport_categories,\"file_name\")\n",
    "            features_subset = features_subset.join(frac_udp_appport_categories,\"file_name\")\n",
    "\n",
    "        # IP address entropies\n",
    "        for name in ['ip_dst','ip_src']:\n",
    "            octets = F.split(datadf[name], '\\.')\n",
    "            datadf = datadf.withColumn(name + '_1st_octet', octets.getItem(0))\n",
    "            datadf = datadf.withColumn(name + '_2nd_octet', octets.getItem(1))\n",
    "            datadf = datadf.withColumn(name + '_3rd_octet', octets.getItem(2))\n",
    "            datadf = datadf.withColumn(name + '_4th_octet', octets.getItem(3))\n",
    "            entropy_1st_IP_octet = datadf.agg(entropy_udf(col(name + '_1st_octet')).alias(\"entropy_1st_IP_octet(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "            entropy_2nd_IP_octet = datadf.agg(entropy_udf(col(name + '_2nd_octet')).alias(\"entropy_2nd_IP_octet(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "            entropy_3rd_IP_octet = datadf.agg(entropy_udf(col(name + '_3rd_octet')).alias(\"entropy_3rd_IP_octet(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "            entropy_4th_IP_octet = datadf.agg(entropy_udf(col(name + '_4th_octet')).alias(\"entropy_4th_IP_octet(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "            features_subset = features_subset.join(entropy_1st_IP_octet,\"file_name\")\n",
    "            features_subset = features_subset.join(entropy_2nd_IP_octet,\"file_name\")\n",
    "            features_subset = features_subset.join(entropy_3rd_IP_octet,\"file_name\")\n",
    "            features_subset = features_subset.join(entropy_4th_IP_octet,\"file_name\")\n",
    "\n",
    "        for name in [\"tcp_flags_ack\", \"tcp_flags_cwr\", \"tcp_flags_fin\", \"tcp_flags_ecn\", \"tcp_flags_ns\", \"tcp_flags_push\", \"tcp_flags_syn\", \"tcp_flags_urg\",\"tcp_flags_reset\"]:\n",
    "            frac_tcpflag_result = datadf.agg(frac_tcpflag(name).alias(\"frac_tcpflag(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "            num_tcpflag_result = datadf.agg(num_tcpflag(name).alias(\"num_tcpflag(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "            features_subset = features_subset.join(frac_tcpflag_result,\"file_name\")\n",
    "            features_subset = features_subset.join(num_tcpflag_result,\"file_name\")\n",
    "\n",
    "        # Fragmentation has occured when either the more fragment bit is set or the fragmentation offset is greater than zero. The filter tp display both types would look like:\n",
    "        # ip.flags.mf ==1 or ip.frag_offset gt 0\n",
    "        # https://osqa-ask.wireshark.org/questions/41152/how-to-check-if-fragmentation-is-happening\n",
    "        datadf = datadf.withColumn('fragmentation', datadf.ip_flags_mf + datadf.ip_frag_offset)\n",
    "        datadf = datadf.withColumn('fragmentation(binary)', F.when(F.col('fragmentation') > 0, 1).otherwise(0))\n",
    "        name = 'fragmentation(binary)'\n",
    "        frac_frag_result = datadf.agg(frac_tcpflag(name).alias(\"frac_frag\")).withColumn(\"file_name\", lit(file_name))\n",
    "        num_frag_result = datadf.agg(num_tcpflag(name).alias(\"num_frag\")).withColumn(\"file_name\", lit(file_name))\n",
    "        features_subset = features_subset.join(frac_frag_result,\"file_name\")\n",
    "        features_subset = features_subset.join(num_frag_result,\"file_name\")\n",
    "\n",
    "        name = \"icmp_type\"\n",
    "        icmp_type_entropy_result = datadf.agg(entropy_udf(name).alias(\"entropy(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "        frac_echo_reply_destination_unreachable_redirect_message_echo_request_result = datadf.agg(frac_echo_reply_destination_unreachable_redirect_message_echo_request(name).alias(\"frac_echo_reply_destination_unreachable_redirect_message_echo_request(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "        num_echo_reply_destination_unreachable_redirect_message_echo_request_result = datadf.agg(num_echo_reply_destination_unreachable_redirect_message_echo_request(name).alias(\"num_echo_reply_destination_unreachable_redirect_message_echo_request(\"+name+\")\")).withColumn(\"file_name\", lit(file_name))\n",
    "\n",
    "        features_subset = features_subset.join(icmp_type_entropy_result,\"file_name\")\n",
    "        features_subset = features_subset.join(frac_echo_reply_destination_unreachable_redirect_message_echo_request_result,\"file_name\")\n",
    "        features_subset = features_subset.join(num_echo_reply_destination_unreachable_redirect_message_echo_request_result,\"file_name\")        \n",
    "        try:\n",
    "            features_subset.show()\n",
    "            #features_subset.write.save(path=output_loc, format='csv', mode='append', sep='\\t')\n",
    "        except:\n",
    "            print(file_name + \" not processed\")\n",
    "    except:\n",
    "        print(file_name + \" not processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Appendix - Feature extraction script running - possible java issues\n",
    "Java memory runs out sometimes on the servers during the processing of tables. Most probably this is due to fact, that Spark is not designed to process large number of files in loop, but rather in bulk - Big data :).\n",
    "So the script crashes occasionally.\n",
    "Workaround is to run it in loop (if it unexpectedly freezes) and additionally restart it if it crashes with JAVA error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# script to run the feature extraction in loop, called \"neverending_script.sh\"\n",
    "# run it as \"nohup neverending_script.sh &> neverending_script_nohup.out&\" so it runs in background and the stdout goes to file\n",
    "%bash\n",
    "#!/bin/bash\n",
    "while true\n",
    "do\n",
    "        nohup ./mawi_feature_extraction.py &\n",
    "        wait\n",
    "        sleep 1\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# script to kill/restart the neverending_script.sh, called \"neverending_script_restarter.sh\"\n",
    "# it checks neverending_script_nohup.out to see if there are JAVA memory issues\n",
    "# run it as \"nohup neverending_script_restarter.sh &> neverending_script_restarter_nohup.out&\"\n",
    "%bash\n",
    "#!/bin/bash\n",
    "\n",
    "# continuosly check the last line in neverwnding script output for jabva out of memory pattern\n",
    "# if it finds anything ($? true value of previous output, 0=true, 1=false then it will kill the process which will restart the process in the \"neverendingscript.py\n",
    "\n",
    "tail -fn0 neverending_script_nohup.out | \\\n",
    "while read line ; do\n",
    "        echo \"$line\" | grep \"java.lang.OutOfMemoryError\"\n",
    "        if [ $? = 0 ]\n",
    "        then\n",
    "                pkill -f /.../mawi_feature_extraction.py\n",
    "                echo \"mawi_feature_extraction.py killed\"\n",
    "        fi\n",
    "done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
